# 文本生成视频AI模型 Sora 技术解读
<img src="images/Sora.jpg" alt="Sora" width="500"/>

## Sora 简介
Sora是由OpenAI研发的Text-to-Video模型，可以看作是GPT2 -> GPT3的飞跃。Sora的核心思想就是堆数据堆算力，即利用大规模的视频数据以及非常多的算力训练视频生成模型，类似的思路已经在LLM中得到验证。具体而言，在数据方面，使用不同时长、不同分辨率以及不同高宽比的视频和图像数据训练文本条件扩散模型；模型方面，使用Transformer结构对视频和图像的时空patch的潜在编码进行操作。他们通过实验得出：大规模的视频生成模型是构建物理世界通用模拟器的一条很有前途的途径。技术报告原文为[video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)，然而比较遗憾的是，并未说明如何构建数据，如何训练模型以及有哪些trick。

## 相关工作
目前已经有很多工作研究了如何对生成视频进行建模，包括RNN，GAN，自回归Transformer以及扩散模型。这些工作通常关注一小类视觉数据、较短的视频或固定大小的视频。Sora是一个通用的视觉数据模型，它可以生成不同时长、高宽比和分辨率的视频和图像，最高可达一分钟的高清视频。

## 一些技术细节

### 视觉数据如何表示——转换成patch

LLM范式的成功在一定程度上得益于token的使用，token优雅地统一了文本的各种形式——代码、数学和各种自然语言。因此，他们考虑如何让视频生成模型继承这些优点，根据之前大量的工作，patch被证明是视觉数据有效的表示，例如[ViT](https://arxiv.org/pdf/2010.11929.pdf)，因此他们选择使用patch作为视频生成模型的基本单元。他们通过实验发现，使用patch表示视频和图像数据训练视频生成模型具有高度的可扩展性和有效性。他们首先将视频利用视觉编码器压缩到较低维度的潜在空间中，随后将表示分解为时空patch。

<img src="images/visual_data_representation.jpg" alt="visual_data_representation" width="500"/>

### 视频压缩网络

他们训练了一个降低视觉数据维度的网络，该网络将原始视频作为输入，并输出在时间和空间上都被压缩的潜在表示。Sora在压缩的潜在空间中被训练生成视频。他们还训练了一个相应的解码器模型，该模型将生成的潜在表示映射回像素空间。

### 时空潜在patch

对于压缩的输入视频，他们提取出一个时空patch序列作为Transformer的token。这种方案也适用于图像，因为图像是一个单帧视频。基于patch的表示使得Sora能够在不同分辨率、时长和高宽比的视频和图像上进行训练。在推理时，可以在适当大小的网格中排列随机初始化的patch来控制生成视频的大小。

### 用于视频生成的缩放Transformer

Sora使用了[Diffusion Transformer（DiT）](https://arxiv.org/pdf/2212.09748.pdf)作为模型结构，对于输入的噪声patch，以及类似文本prompt的条件信息，Sora被用于训练生成原始视频的patch。之所以选择Transformer，是因为Transformer已经被证明在许多领域都有显著的缩放特性，例如语言建模，计算机视觉以及图像生成。他们通过实验发现，随着计算资源的增加，生成视频的质量显著提升。

<img src="images/DiT.jpg" alt="DiT" width="500"/>

### 可变时长、分辨率、高宽比

之前的图像和视频生成方法通常将视频调整或裁剪为标准大小，例如256x256分辨率的视频。但是他们发现使用原始数据大小进行训练有几个好处。

- 采样更加灵活：Sora可以对分辨率为1920x1080或1080x1920的视频以及其间所有分辨率的视频进行采样。这使得Sora可以直接按照不同设备的固有高宽比为其创建内容。
- 提升视频中的构图和取景：他们通过实验发现，以视频的固有高宽比进行视频的训练可以改善构图和取景。他们将两种视频处理方式（一种是将所有训练视频裁剪为正方形，一种是使用原始视频）训练的模型进行比较，发现Sora可以更好地生成视频。

### 提升模型对文本的理解

训练Text-to-Video模型需要大量具有对应文本字幕的视频。为了获得高质量的text-videos对数据，他们训练一个字幕生成模型，然后使用它为训练集中的所有视频生成文本字幕。他们发现，基于高度描述性的视频字幕进行训练可以提高文本保真度以及视频的整体质量。同时，他们还发现，使用GPT将用户的prompt转换成更长的，更细节的描述送入到视频生成模型中。使得Sora能够准确地按照用户提示生成高质量的视频。

### 使用图像和视频进行提示

除了文本以外，Sora可以使用图像或视频作为提示，这使得Sora能够执行很多图像和视频编辑任务，例如创建循环的视频，为静态图像生成动画以及在时间维度上向前或向后扩展视频等。

- 图像动画制作：根据提供的图像和文本提示生成视频。
- 拓展生成的视频：在时间维度上向前或向后扩展视频。
- 视频到视频编辑：他们将[SDEdit](https://arxiv.org/pdf/2108.01073.pdf)方法应用于Sora，可以以零样本的方式对输入视频的风格和环境进行转换。
- 连接视频：可以使用Sora在两个输入视频之间逐渐插值，在具有完全不同主题和场景组成的视频之间创建无缝过渡视频片段。

### 生成图像能力

Sora可以通过在时间范围为一帧的空间网格中排列高斯噪声patch来实生成各种尺寸的图像，分辨率高达2048x2048。

### Sora的整体模型结构

参考网上大神画的图，Sora的整体模型结构如下：

<img src="images/Sora_model.jpg" alt="Sora_model" width="500"/>

其中需要注意几点：
- 在Conditioning阶段可能不是一帧对应一个文本，而可能是几帧十几帧对应一段文本描述。
- 在编码成Spacetime latent patches的时候可能用到了[ViViT](https://arxiv.org/pdf/2103.15691.pdf)的时空编码方式。
- 输入给Decoder的内容应该是去噪之后的patches序列。

## 涌现的世界模拟能力

他们发现，视频模型在大规模训练时表现出许多有趣的涌现能力。这些功能使Sora能够从物理世界模拟人、动物和环境的某些方面。

- 3D一致性：Sora可以生成具有动态摄像机运动的视频。随着摄像机的移动和旋转，人物和场景元素在三维空间中保持一致移动。
- 长程一致性和物体永久性：对于视频生成系统来说，一个重要的挑战是在采样长视频时保持时间上的一致性。我们发现，Sora通常能够有效地建模短程和长程的依赖关系，尽管并非总是如此。例如，我们的模型可以在人、动物和物体被遮挡或离开画面时仍然保持它们的存在。同样，它可以在一个样本中生成同一角色的多个镜头，并在整个视频中保持它们的外观。
- 与世界互动：Sora有时可以模拟对世界产生简单影响的动作。例如，画家可以在画布上留下持续存在的新笔触，或者一个人可以吃掉一个汉堡并留下咬痕。
- 模拟数字世界：Sora还能够模拟人工过程，一个例子是视频游戏。Sora可以同时使用基本策略控制Minecraft中的玩家，同时以高保真度渲染世界及其动态。通过提示Sora提到“Minecraft”的标题，可以激发这些能力。

这些能力表明，视频模型的持续扩展是开发物理和数字世界以及生活在其中的物体、动物和人的高效模拟器的一条很有前途的道路。
